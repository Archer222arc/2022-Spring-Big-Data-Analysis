\documentclass{article}
%\usepackage{style}
\usepackage[ruled, algo2e]{algorithm2e}
\input{preamble.tex}
%\usepackage{lineno}
%\linenumbers
\usepackage{caption}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\mL}{\mathcal L}
\newcommand{\dA}{\Delta(\A)}
\newcommand{\dB}{\Delta(\B)}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\dualgap}{\Operatorname{DualGap}}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\proj}{\mathrm{proj}}
\title{Randomized Singular Value Decomposition Algorithm}
\date{\today}	
\author{Ruicheng Ao 1900012179}
\begin{document}
\maketitle
\section{Problem description}
For a matrix $A\in\R^{m\times n}$, its singular value decomposition (SVD) can be formulated as 
\begin{equation}\label{svd}
    A = U\Sigma V^\top = \sum_{k=1}^{\min(m,n)}\sigma_ku_kv_k^\top,
\end{equation}
where $\sigma_1\ge\sigma_2\ge\dots,\sigma_{\min(m,n)}\ge 0$ are so-called singular values of $A$ and $U,V$ have orthogonal columns. In some situations, it is requisited to know the $k$ largest singular values of $A$. Traditional methods based on numerical algebra can obtain such decomposition with a cost of $\Ocal(mnk)$ \cite{golub2013matrix}. To further reduce the time cost, many randomized procedures are investigated. We mainly focus on two of them, namely, Linear Time SVD \cite{drineas2006fast} and Prototype Randomized SVD \cite{halko2011finding}.
\section{Algorithm description}
In this section, we give detailed descriptions about the two SVD algorithms mentioned above.
\subsection{Linear Time SVD}
The linear time SVD approximate $A\in\R^{m\times n}$ with a matrix $C\in\R^{m\times c}$, where $c$ only depends on $k$. The columns of $C$ are randomly sampled from those of $A$ according to importance. The general framework is given in Algorithm \ref{linear}.
\begin{algorithm}[p]
    \caption{Linear Time SVD}
    \begin{algorithmic}[1]\label{linear}
        \REQUIRE{$A\in\R^{m\times n}$, number of samples $c$}
        \STATE{Set importance weight $p_i=\|A_i\|_2^2/\|A\|_F^2$ for sampling}
        \FOR{$l=1,2,\dots,c$}
        \STATE{Select $i_l\in\{1,2,\dots,n\}$ with probability distribution $\P(i_l=j)=p_j$}
        \STATE{Set $C_l=A_{i_l}/\sqrt{cp_{i_l}}$}
        \ENDFOR
        \STATE{Compute the $k-$largest sigular values of $C=U_c\Sigma_cV_c^\top$}
        \IF{Postprocessing}
        \STATE{Set $Y=A^\top U_c$}
        \STATE{Compute QR decomposition $Y=QR$}
        \STATE{Compute SVD for $R^\top=U_r\Sigma_rV_r^\top$}
        \STATE{Set $U_k=U_cU_r,V_k=QV_r$}
        \ENDIF
        \RETURN{The approximate $k-$largest singular values $\Sigma_k=\Sigma_r$ with left and right singular values $U_k,V_k$.}
    \end{algorithmic}
\end{algorithm}
\subsection{Propotype Randomized SVD}
The Prototype Randomized SVD method utilizes random sampling to identify a subsapce that captures most characteristics of a matrix. The sampling reduces the computational cost by projection through an orthogonal matrix obtained from the QR decomposition. After that, the reduced matrix is directly manipulated to obtain the decomposition. We demonstrate the framework in Algorithm \ref{proto}
\begin{algorithm}[p]
    \caption{Prototype Randomized SVD}
    \begin{algorithmic}[1]\label{proto}
        \REQUIRE{$A\in\R^{m\times n}$, preprocessing exponential $q$, number of additional sampling $p$}
        \STATE{Generate a Gaussian test matrix $\Omega\in\R^{n\times (k+p)}$}
        \STATE{Set $Y=(AA^\top)^qA\Omega$}
        \STATE{Compute QR decomposition $Y=QR$}
        \STATE{Projection $B=Q^\top A$}
        \STATE{Compute SVD $B=U_B\Sigma_BV_BT^\top$}
        \STATE{Set $U_k = QU_B$}
        \RETURN{The $k-$largetst singular values $\Sigma_k=\Sigma_B$ with the left and right vectors $U_k,V_k=V_B$}
    \end{algorithmic}
\end{algorithm}
\section{Numerical experiments}
In this section, we test the above two algorithms on two datasets to see the efficiency of SVD. Then they are applied to accelerate the matrix completion problem.

\emph{Synthetic datasets} The first dataset is generated by the matlab code 
$$
\begin{aligned}
&m=2048 ; \\
&n=512 ; \\
&p=20 ; \\
&A=r a n d n(m, p) \star r a n d n(p, n) ;
\end{aligned}
$$
The second comes from \cite{halko2011finding} by utilizing its code from github (not copy directly). We test our algorithms on 2 of the distributions for comparison (required 1). The matrix size is chosen to be $4096\times 4096$ in dataset 2. The parameters are chosen to be $c=2k,10k,50k$ for linear SVD and $q=0,1,2$ for Prototype SVD. The parameter $p$ in Algorithm \ref{proto} is set to be $k$, which doubles the sizes of sampling matrix. For each $A$, the number of the largest eigenvalues we compute is chosen to be $k=5,10,15,20$.

The results are displayed as below. Figures 1-12 show the magnitude of each eigenvalues compared with the MATLAB default function $svds$. The tables 1-12 record the relative error of eigenvalues and the corresponding eigen vectors, as well as the CPU time compared with the baseline $svds$.
\begin{figure}[p]
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_5.png}
		\caption{$k=5$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_10.png}
		\caption{$k=10$}
	\end{minipage}
\end{figure} 

    \begin{figure}[p]
    \begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_15.png}
		\caption{$k=15$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_20.png}
		\caption{$k=20$}
	\end{minipage}
	\caption*{Results on dataset 1 with different $k$}
\end{figure}
\input{./table/dataset1_time.tex}
\input{./table/dataset1_sigma.tex}
\input{./table/dataset1_U.tex}
\input{./table/dataset1_V.tex}  
\begin{figure}[p]
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k5_t1.png}
		\caption{$k=5$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k10_t1.png}
		\caption{$k=10$}
	\end{minipage}
    \begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k15_t1.png}
		\caption{$k=15$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k20_t1.png}
		\caption{$k=20$}
	\end{minipage}
	\caption*{Results on dataset 2, distribution 1 with different $k$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k5_t2.png}
		\caption{$k=5$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k10_t2.png}
		\caption{$k=10$}
	\end{minipage}
    \begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k15_t2.png}
		\caption{$k=15$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset2_k20_t2.png}
		\caption{$k=20$}
	\end{minipage}
	\caption*{Results on dataset 2, distribution 1 with different $k$}
\end{figure} 
\input{./table/dataset2_time_t=1.tex}
\input{./table/dataset2_time_t=2.tex}
\input{./table/dataset2_sigma_t=1.tex}
\input{./table/dataset2_sigma_t=2.tex}
\input{./table/dataset2_U_t=1.tex}
\input{./table/dataset2_U_t=2.tex}
\input{./table/dataset2_V_t=1.tex}
\input{./table/dataset2_V_t=2.tex}

We observe that the Linear time SVD works not very well on dataset 1 when $c$ is small, however, it shows good performance when $k$ or $c$ is large. Both algorithms behave well on dataset 2, from that the tables show that they can exactly recover the eigenvalues even when $k=5$. An explanation for better behavior when $k$ is larger is that larger $k$ results in larger sample sets, which leads to better simulation of $A$. Another interpretation is that the rank of $A$ is exactly $20$ in the first dataset, thus when $k\ge 10$, the restoration of the information of $A$ is complete for the Prototype SVD and $k=20$ for Linear time SVD.

On the other hand, we find that for problems of small scale (rank) like dataset 1, the random SVD has little advantage, while for much larger matrix in dataset 2, the random algorithms outperform the baseline MATLAB SVD, which is corresponding to our intuition that, it is hard to defeat MATLAB when the problem is of small scale, while we can do much better for larger problem.
\bibliographystyle{plain}  
\bibliography{ref}
\end{document}