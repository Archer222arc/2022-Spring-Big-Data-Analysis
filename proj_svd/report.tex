\documentclass{article}
%\usepackage{style}
\usepackage[ruled, algo2e]{algorithm2e}
\input{preamble.tex}
%\usepackage{lineno}
%\linenumbers
\usepackage{caption}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\mL}{\mathcal L}
\newcommand{\dA}{\Delta(\A)}
\newcommand{\dB}{\Delta(\B)}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\dualgap}{\Operatorname{DualGap}}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\proj}{\mathrm{proj}}
\title{Randomized Singular Value Decomposition Algorithm}
\date{\today}	
\author{Ruicheng Ao 1900012179}
\begin{document}
\maketitle
\section{Problem description}
For a matrix $A\in\R^{m\times n}$, its singular value decomposition (SVD) can be formulated as 
\begin{equation}\label{svd}
    A = U\Sigma V^\top = \sum_{k=1}^{\min(m,n)}\sigma_ku_kv_k^\top,
\end{equation}
where $\sigma_1\ge\sigma_2\ge\dots,\sigma_{\min(m,n)}\ge 0$ are so-called singular values of $A$ and $U,V$ have orthogonal columns. In some situations, it is requisited to know the $k$ largest singular values of $A$. Traditional methods based on numerical algebra can obtain such decomposition with a cost of $\Ocal(mnk)$ \cite{golub2013matrix}. To further reduce the time cost, many randomized procedures are investigated. We mainly focus on two of them, namely, Linear Time SVD \cite{drineas2006fast} and Prototype Randomized SVD \cite{halko2011finding}.
\section{Algorithm description}
In this section, we give detailed descriptions about the two SVD algorithms mentioned above.
\subsection{Linear Time SVD}
The linear time SVD approximate $A\in\R^{m\times n}$ with a matrix $C\in\R^{m\times c}$, where $c$ only depends on $k$. The columns of $C$ are randomly sampled from those of $A$ according to importance. The general framework is given in Algorithm \ref{linear}.
\begin{algorithm}[p]
    \caption{Linear Time SVD}
    \begin{algorithmic}[1]\label{linear}
        \REQUIRE{$A\in\R^{m\times n}$, number of samples $c$}
        \STATE{Set importance weight $p_i=\|A_i\|_2^2/\|A\|_F^2$ for sampling}
        \FOR{$l=1,2,\dots,c$}
        \STATE{Select $i_l\in\{1,2,\dots,n\}$ with probability distribution $\P(i_l=j)=p_j$}
        \STATE{Set $C_l=A_{i_l}/\sqrt{cp_{i_l}}$}
        \ENDFOR
        \STATE{Compute the $k-$largest sigular values of $C=U_c\Sigma_cV_c^\top$}
        \IF{Postprocessing}
        \STATE{Set $Y=A^\top U_c$}
        \STATE{Compute QR decomposition $Y=QR$}
        \STATE{Compute SVD for $R^\top=U_r\Sigma_rV_r^\top$}
        \STATE{Set $U_k=U_cU_r,V_k=QV_r$}
        \ENDIF
        \RETURN{The approximate $k-$largest singular values $\Sigma_k=\Sigma_r$ with left and right singular values $U_k,V_k$.}
    \end{algorithmic}
\end{algorithm}
\subsection{Propotype Randomized SVD}
The Prototype Randomized SVD method utilizes random sampling to identify a subsapce that captures most characteristics of a matrix. The sampling reduces the computational cost by projection through an orthogonal matrix obtained from the QR decomposition. After that, the reduced matrix is directly manipulated to obtain the decomposition. We demonstrate the framework in Algorithm \ref{proto}
\begin{algorithm}[p]
    \caption{Prototype Randomized SVD}
    \begin{algorithmic}[1]\label{proto}
        \REQUIRE{$A\in\R^{m\times n}$, preprocessing exponential $q$, number of additional sampling $p$}
        \STATE{Generate a Gaussian test matrix $\Omega\in\R^{n\times (k+p)}$}
        \STATE{Set $Y=(AA^\top)^qA\Omega$}
        \STATE{Compute QR decomposition $Y=QR$}
        \STATE{Projection $B=Q^\top A$}
        \STATE{Compute SVD $B=U_B\Sigma_BV_BT^\top$}
        \STATE{Set $U_k = QU_B$}
        \RETURN{The $k-$largetst singular values $\Sigma_k=\Sigma_B$ with the left and right vectors $U_k,V_k=V_B$}
    \end{algorithmic}
\end{algorithm}
\section{Numerical experiments}
In this section, we test the above two algorithms on two datasets to see the efficiency of SVD. Then they are applied to accelerate the matrix completion problem.

\subsection{Comparison of SVD and image restoration}
\emph{Synthetic datasets} The first dataset is generated by the matlab code 
$$
\begin{aligned}
&m=2048 ; \\
&n=512 ; \\
&p=20 ; \\
&A=r a n d n(m, p) \star r a n d n(p, n) ;
\end{aligned}
$$
The second example is our self-chosen image of size $1080\times 1920$. We convert it into grey scale matrix and apply partial SVD with different $k$ on it to see the performance and restoration effact. For the first example, we compare the performance of Linear time with $c=2k,10k,50k$ and Prototype $q=0,1,2$, while for the second example, we mainly focus on the influence of different $k$ on recovery of the image, which is the most important problem in image processing. $c=10k$ is chosen in example 2.
% The second comes from \cite{halko2011finding} by utilizing its code from github (not copy directly). We test our algorithms on 2 of the distributions for comparison (required 1). The matrix size is chosen to be $4096\times 4096$ in dataset 2. The parameters are chosen to be $c=2k,10k,50k$ for linear SVD and $q=0,1,2$ for Prototype SVD. The parameter $p$ in Algorithm \ref{proto} is set to be $k$, which doubles the sizes of sampling matrix. For each $A$, the number of the largest eigenvalues we compute is chosen to be $k=5,10,15,20$.


The results are displayed as below. Figures 1-4 show the magnitude of each eigenvalues compared with the MATLAB default function $svds$ on dataset 1. Figures \ref{example21}-\ref{example22} show the recovery of image with different $k$. The tables 1-12 record the relative error of eigenvalues and the corresponding eigen vectors, as well as the CPU time compared with the baseline $svds$.
\begin{figure}[p]
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_5.png}
		\caption{$k=5$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_10.png}
		\caption{$k=10$}
	\end{minipage}
\end{figure} 

    \begin{figure}[p]
    \begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_15.png}
		\caption{$k=15$}
	\end{minipage}
	\begin{minipage}{0.54\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/dataset1_20.png}
		\caption{$k=20$}
	\end{minipage}
	\caption*{Results on dataset 1 with different $k$}
\end{figure}
\input{./table/dataset1_time.tex}
\input{./table/dataset1_sigma.tex}
\input{./table/dataset1_U.tex}
\input{./table/dataset1_V.tex}  
\input{./table/image1.tex}
\input{./table/image2.tex}
\input{./table/image3.tex}
\input{./table/image4.tex}
\input{./table/image5.tex}
\input{./table/image6.tex}
\begin{figure}[H]
	\centering 
	\includegraphics[width=12cm]{./fig/image_gray.jpeg}
	\caption{The realistic image}
\end{figure}
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec11.jpeg}
		\caption{Linear time\label{example21}}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec12.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec13.jpeg}
		\caption{SVDS }
	\end{minipage}
	\caption*{Results on realistic image, $k=5$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec11.jpeg}
		\caption{Linear time}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec12.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec13.jpeg}
		\caption{SVDS }
	\end{minipage}
	\caption*{Results on realistic image, $k=10$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec21.jpeg}
		\caption{Linear time}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec22.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec23.jpeg}
		\caption{SVDS }
	\end{minipage}
	\caption*{Results on realistic image, $k=15$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec31.jpeg}
		\caption{Linear time}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec32.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec33.jpeg}
		\caption{SVDS }
	\end{minipage}
	\caption*{Results on realistic image, $k=20$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec41.jpeg}
		\caption{Linear time}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec42.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec43.jpeg}
		\caption{SVDS }
	\end{minipage}
	\caption*{Results on realistic image, $k=100$}
\end{figure} 
\begin{figure}[p]
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec51.jpeg}
		\caption{Linear time}
	\end{minipage}
	\begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec52.jpeg}
		\caption{Prototype}
	\end{minipage}
    \begin{minipage}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{./fig/image_rec53.jpeg}
		\caption{SVDS \label{example22}}
	\end{minipage}
	\caption*{Results on realistic image, $k=200$}
\end{figure} 
% \begin{figure}[p]
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k5_t1.png}
% 		\caption{$k=5$}
% 	\end{minipage}
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k10_t1.png}
% 		\caption{$k=10$}
% 	\end{minipage}
%     \begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k15_t1.png}
% 		\caption{$k=15$}
% 	\end{minipage}
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k20_t1.png}
% 		\caption{$k=20$}
% 	\end{minipage}
% 	\caption*{Results on dataset 2, distribution 1 with different $k$}
% \end{figure} 
% \begin{figure}[p]
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k5_t2.png}
% 		\caption{$k=5$}
% 	\end{minipage}
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k10_t2.png}
% 		\caption{$k=10$}
% 	\end{minipage}
%     \begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k15_t2.png}
% 		\caption{$k=15$}
% 	\end{minipage}
% 	\begin{minipage}{0.54\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{./fig/dataset2_k20_t2.png}
% 		\caption{$k=20$}
% 	\end{minipage}
% 	\caption*{Results on dataset 2, distribution 1 with different $k$}
% \end{figure} 
% \input{./table/dataset2_time_t=1.tex}
% \input{./table/dataset2_time_t=2.tex}
% \input{./table/dataset2_sigma_t=1.tex}
% \input{./table/dataset2_sigma_t=2.tex}
% \input{./table/dataset2_U_t=1.tex}
% \input{./table/dataset2_U_t=2.tex}
% \input{./table/dataset2_V_t=1.tex}
% \input{./table/dataset2_V_t=2.tex}

We observe that the Linear time SVD works not very well on dataset 1 when $c$ is small, however, it shows good performance when $k$ or $c$ is large. All algorithms behave well on realistic image, from that the tables show that they can exactly recover the eigenvalues even when $k=5$. An explanation for better behavior when $k$ is larger is that larger $k$ results in larger sample sets, which leads to better simulation of $A$. Another interpretation is that the rank of $A$ is exactly $20$ in the first dataset, thus when $k\ge 10$, the restoration of the information of $A$ is complete for the Prototype SVD and $k=20$ for Linear time SVD.

On the other hand, we find that for problems of small scale (rank) like dataset 1, the random SVD has little advantage, while for much larger matrix in the realistic image, the random algorithms outperform the baseline MATLAB SVD, which is corresponding to our intuition that, it is hard to defeat MATLAB when the problem is of small scale, while we can do much better for larger problem.

The relative error in $U,V$ does not mean that the approximate performance is not good, but it may due to space representation, as we see the error in singular value is relatively low. On the other hand, the recovery result of our proposed algorithms is also really good, which looks better than the baseline. This proves its practical use. Additionally, we find that about $200$ singular values can recover a realistic image of size $1920\times 1080$.
\subsection{Accelerate low-rank matrix recovery}
We apply the partial SVD of Linear time algorithm and Prototype algorithm with $k=5$ on the lrmr problem
\begin{equation}
	\min _{X \in R^{m \times n}} \frac{1}{2} \sum_{(i, j) \in \Omega}\left(X_{i j}-M_{i j}\right)^{2}+\mu\|X\|_{*} .
	\end{equation}
	The settings follow those in midterm project. We list the results in Table \ref{example1},\ref{example2}.
	\begin{table}
		\centering
		\begin{tabular}{|c|c|c|c|c|c|}
		
		\hline
		\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.1}\\\cline{2-6}
		 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
		Mosek & $1.67e+00$ & $1.19e+01$ & $5.43e-03$ & $1.58e-01$ & $1.19e+02$\\\hline
		ADMM & $2.51e-01$ & $1.19e+01$ & $5.47e-03$ & $1.73e-01$ & $1.19e+02$\\\hline
		ADMM linear time & $2.19e-01$ & $1.24e+01$ & $1.73e-02$ & $8.04e-02$ & $1.19e+02$\\\hline
		ADMM prototype & $2.02e-01$ & $1.19e+01$ & $5.83e-03$ & $1.42e-01$ & $1.19e+02$\\\hline
		\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.01}\\\cline{2-6}
 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
Mosek & $2.09e+00$ & $1.20e+00$ & $5.76e-04$ & $1.56e-01$ & $1.20e+02$\\\hline
ADMM & $2.09e-01$ & $1.20e+00$ & $5.79e-04$ & $2.02e-01$ & $1.20e+02$\\\hline
ADMM linear time & $1.72e-01$ & $1.80e+00$ & $1.87e-02$ & $6.60e-02$ & $1.21e+02$\\\hline
ADMM prototype & $1.43e-01$ & $1.20e+00$ & $8.82e-04$ & $1.52e-01$ & $1.20e+02$\\\hline
\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.001}\\\cline{2-6}
 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
Mosek & $1.11e+00$ & $1.20e-01$ & $5.77e-05$ & $1.56e-01$ & $1.20e+02$\\\hline
ADMM & $2.06e-01$ & $1.20e-01$ & $5.91e-05$ & $2.50e-01$ & $1.20e+02$\\\hline
ADMM linear time & $1.49e-01$ & $1.84e-01$ & $6.06e-03$ & $5.49e-02$ & $1.21e+02$\\\hline
ADMM prototype & $1.31e-01$ & $1.20e-01$ & $9.84e-05$ & $1.59e-01$ & $1.20e+02$\\\hline
		\end{tabular}
		\caption{Numerical result on example 1 with different $\mu$\label{example1}}
		
		\end{table}
		\begin{table}
			\centering
			\begin{tabular}{|c|c|c|c|c|c|}
			
			\hline
			\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.1}\\\cline{2-6}
			 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
			Mosek & $9.04e+00$ & $1.99e+01$ & $1.86e-03$ & $2.33e-03$ & $1.99e+02$\\\hline
			ADMM & $8.25e-01$ & $1.99e+01$ & $1.86e-03$ & $2.33e-03$ & $1.99e+02$\\\hline
			ADMM linear time & $8.45e-01$ & $2.00e+01$ & $2.55e-03$ & $2.95e-03$ & $1.99e+02$\\\hline
			ADMM prototype & $2.91e-01$ & $1.99e+01$ & $1.86e-03$ & $2.33e-03$ & $1.99e+02$\\\hline
			\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.01}\\\cline{2-6}
 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
Mosek & $1.09e+01$ & $2.00e+00$ & $1.86e-04$ & $2.33e-04$ & $2.00e+02$\\\hline
ADMM & $6.68e-01$ & $2.00e+00$ & $1.86e-04$ & $2.33e-04$ & $2.00e+02$\\\hline
ADMM linear time & $6.65e-01$ & $2.00e+00$ & $2.50e-04$ & $2.82e-04$ & $2.00e+02$\\\hline
ADMM prototype & $3.11e-01$ & $2.00e+00$ & $1.86e-04$ & $2.33e-04$ & $2.00e+02$\\\hline
\multirow{2}{*}{ Algorithm} &\multicolumn{5}{c|}{$\mu=$0.001}\\\cline{2-6}
 &CPU time &Opt value &Error with M in norm $\|\cdot\|_\F$ &Error with $A$ in norm $\|\cdot\|_\F$ &$\|x\|_*$\\\hline
Mosek & $1.19e+01$ & $2.00e-01$ & $1.86e-05$ & $2.37e-05$ & $2.00e+02$\\\hline
ADMM & $1.31e+00$ & $2.00e-01$ & $2.99e-05$ & $1.49e-02$ & $2.00e+02$\\\hline
ADMM linear time & $6.91e-01$ & $2.00e-01$ & $2.50e-05$ & $2.91e-05$ & $2.00e+02$\\\hline
ADMM prototype & $2.56e-01$ & $2.00e-01$ & $1.86e-05$ & $2.33e-05$ & $2.00e+02$\\\hline
	\end{tabular}
	\caption{Numerical result on example 2 with different $\mu$\label{example2}}
	\end{table}
			
It should be mentioned that the acceleration is obvious compared with the default SVD in MATLAB. Also, despite more time Linear time algorithm has spent than prototype (though still faster than vanilla SVD), the much better recovery with primitive matrix is obtained. On the other hand, our proposed SVD also maintains low nuclear norm and low error with $M$, as well as opt value. Therefore, our acceleration by applying SVD techniques mentioned above is truly successful!
		
		
\bibliographystyle{plain}  
\bibliography{ref}
\end{document}